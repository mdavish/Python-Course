{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Web Scraping\n",
    "----\n",
    "One of Python's most useful capabilities is **web scraping** - the systematic extraction and organization of data from around the web. Web scrabing harnesses the multitude of data openly available across the internet and allows us to quickly harvest, organize, and analyze it.\n",
    "\n",
    "Web scraping is used across a variety of firms and industries: \n",
    "- Amazon scrapes the web for competitor prices to ensure that theirs are competitive. \n",
    "- Real estate investors scrape the web for listings in order to closely track market movements. \n",
    "- Recruiters scrape LinkedIn and other professional websites to identify prospects.\n",
    "\n",
    "The most popular libraries used for web scraping are [**BeautifulSoup**](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), [**Scrapy**](https://scrapy.org/), and [**lxml**](http://lxml.de/index.html#documentation). This lesson will teach web-scraping using **BeautifulSoup**, but the concepts behind web-scraping are the same across the different libraries. Once you know one, you'll be able to quickly learn the others. If you haven't already, install it BeautifulSoup now:\n",
    "\n",
    "(If you're connected to an EY WLAN, you will have to install packages directly from the command line to bypass EY's proxy server. More info on this in the attached PowerPoint: _\"Installing Python Packages behind EY Proxy\"_.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll scrape data from [BoxOfficeMojo](http://www.boxofficemojo.com/) - a website that aggregates financial data for movies.\n",
    "\n",
    "![alt text](https://static1.squarespace.com/static/58854b3b6a49636e23dfec7d/t/5a2ebfa28165f502b9ed5ace/1513013155589/BoxOffice_ScreenGrab.PNG)\n",
    "\n",
    "The data on BoxOfficeMojo is an ideal candidate for scraping - it's systematically organized, it doesn't shift much over time, it's largely quantitative, and there's a lot of it.\n",
    "\n",
    "In this lesson, we'll learn to scrape all of the box office data for every single movie on Box Office Mojo. Doing this manually would be tedious and take tens of hours. But with Python, we can do it in a matter of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## HTML Basics\n",
    "--- \n",
    "To understand web-scraping, we'll need a basic understanding HTML and the structure of web-pages. Every web-page on the internet is written in HTML - **hypertext markup language**. HTML isn't a **scripting** language, like Python, which gives step-by-step instructions and manipulates data. \n",
    "\n",
    "HTML is a **markup language** that tells a browser how to layout content and data on a page, similar to the more-simplified **markdown** langauge that this very text is written in. (Double click this cell to view that code.)\n",
    "\n",
    "To view the HTML code behind any web-page, just right click and press **inspect** on your browser (assuming you're using Google Chrome or Firefox). It will look like this:\n",
    "\n",
    "![test](https://static1.squarespace.com/static/58854b3b6a49636e23dfec7d/t/5a2ebb9753450a8ff1cf6750/1513012119547/Inspect_Page_ScreenGrab.PNG)\n",
    "\n",
    "As you can see, beneath every webpage are just lines and lines of HTML code, like the ones shown on the right.\n",
    "\n",
    "Using the **requests** libary, Python can extract this raw code from any webpage. Let's try this with the [yearly page](http://www.boxofficemojo.com/yearly/) on BoxOfficeMojo, which shows some data on the movie market over the last 20+ years.\n",
    "\n",
    "### Note:\n",
    "When using the requests library on an EY network, we'll need to get authentication through EY's **proxy server**. This requires declaring a **proxy** variable as follows:\n",
    "\n",
    "> `proxies = {'http': 'http://EYUser:EYPassword@empweb2.ey.net:8080'}`\n",
    "\n",
    "> (Replace **`EYUser`** and **`EYPassword`** with your EY user and password.)\n",
    "\n",
    "Then, make sure to use this variable as an argument in the **`requests.get()`** function throughout the document. \n",
    "\n",
    "This method only works on EY machines. For client computers, often the proxy server has a different HTTP address (though many clients don't use a proxy server at all.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.boxofficemojo.com', port=443): Max retries exceeded with url: /yearly/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001B1770A1128>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 141\u001b[1;33m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    849\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sock'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 150\u001b[1;33m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x000001B1770A1128>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 )\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[1;32m--> 639\u001b[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[0;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.boxofficemojo.com', port=443): Max retries exceeded with url: /yearly/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001B1770A1128>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-af323cc9eb94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.boxofficemojo.com/yearly/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                     \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m                 )\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.boxofficemojo.com', port=443): Max retries exceeded with url: /yearly/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001B1770A1128>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "proxies = {\n",
    "    'http': 'http://HB711GF:UGHwhatever123@empweb2.ey.net:8080'\n",
    "}\n",
    "\n",
    "page = requests.get('http://www.boxofficemojo.com/yearly/', proxies=proxies).text\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requests library stores the entire HTML page in a huge string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's hard to make sense of all the code in that form. Fortunately, the **BeautifulSoup** library helps organize it into a cleaner format, much like the one you see when you inspect a page on Google chrome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "beautiful_page = BeautifulSoup(page, 'lxml')\n",
    "beautiful_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to examine the structure of this HTML page, because we'll need to understand it in order to understand web scraping. \n",
    "\n",
    "HTML pages, like lots of other data structures, follow a **node/tree** format, where each **node** in the tree belongs to a **parent**, leading up to the **root** node, and each parent has one or a few **children**.\n",
    "\n",
    "If you envisioned a bookstore in this format, it might look something  like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<bookstore>\n",
    "  <book>\n",
    "    <title lang=\"en\">Harry Potter</title>\n",
    "    <author>J K. Rowling</author>\n",
    "    <year>2005</year>\n",
    "    <price>29.99</price>\n",
    "  </book>\n",
    "</bookstore>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example...\n",
    "- The **`<bookstore>`** node is the _root_ node and the *parent* of the **`<book>`** node. \n",
    "- The **`<book>`** node is both the *child* of the **`<bookstore>`** node and the *parent* of the **`<title>`**, **`<author>`**, **`<year>`**, and **`<price>`** nodes.\n",
    "\n",
    "Generally speaking, child nodes are components or descriptors of their parent nodes, like how a **`<book>`** belongs to a **`<bookstore>`** or how the **`<title>`** describes a **`<book>`**.\n",
    "\n",
    "More specifically, there are seven kinds of nodes: \n",
    "1. Elements\n",
    "2. Attributes\n",
    "3. Text\n",
    "4. Namespace\n",
    "5. Processing-instruction\n",
    "6. Comment\n",
    "7. Document\n",
    "\n",
    "In this training, we'll only deal with the first three - elements, attributes, and text - but it's useful to be aware of all the different types. \n",
    "\n",
    "Take the **`<title>`** node, for example. The **`<title>`** node is an **element** node, and it has an **attribute** **`lang=EN`**, and its **text** is **`Harry Potter`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "Try coding an example of EY's strucure in node/tree format:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<EY>\n",
    "    <Region> EMEA </Region>\n",
    "    <Region> Americas </Region>\n",
    "    <Region> FSO </Region>\n",
    "</EY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML follows the same tree format as the examples above - HTML code forms a tree made up of nodes, parents, and children. But in HTML, each node performs a different function, depending on which type of node it is.\n",
    "\n",
    "At the most basic level, the syntax of HTML looks like  this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My First Paragraph</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HTML, each node (which are also referred to as **tags** in HTML) serves a particular purpose:\n",
    "\n",
    "1. **`<!DOCTYPE html>`**: This node declares that the document is in the HTML format, so that the browser knows how to interpret it\n",
    "2. The document itself is contained within the **`<html>`** node \n",
    "3. The **`<body>`** node, which is the child of the **`<html>`** node, encompasses the *visible* part of the HTML document\n",
    "4. HTML headings are defined with the **`<h1>`** to the **`<h6>`** nodes\n",
    "5. HTML paragraphs are defined with the **`<p>`** node\n",
    "6. HTML links are defined with the **`<a>`** tag, wherein the **`href`** attribute contains the link itself. For example:\n",
    "> **`<a href=\"http://www.test.com\">This is a link for test.com</a>`**;\n",
    "7. HTML tables are defined with **`<table>`**, row as **`<tr>`** and rows are divided into individual datapoints as **`<td>`**\n",
    "\n",
    "For example, an HTML table would be coded like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>Jill</td>\n",
    "        <td>Smith</td>\n",
    "        <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Eve</td>\n",
    "        <td>Jackson</td>\n",
    "        <td>94</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, HTML is a great deal more complicated than this basic explanation suggests, and you could take entire courses in HTML and web design alone though. For our purposes, though, it suffices to understand the basic node-tree format of an HTML pages. \n",
    "\n",
    "If you're interested in learning more about HTML, try [this tutorial](https://www.w3schools.com/html/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Beautiful Soup and CSS Querying\n",
    "---\n",
    "\n",
    "Let's revisit the **BeautifulSoup** object we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beautiful_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, by now all of this HTML isn't as confusing as it seemed at first. Nodes have parents and children, all leading up to the **root** node **`<HTML>`**. Links within text are demarcated by **`<href>`** attribute nodes. **`<Table>`** and **`<tr>`** indicate tables and their rows.\n",
    "\n",
    "Of course, we don't need to understand what purpose every single node serves. Some of them call Javascript functions and access hidden APIs - concepts that are beyond the scope of this course. \n",
    "\n",
    "What we're interested in, for this example, is the structured data stored in the main table of the page - and that's where BeautifulSoup comes in.\n",
    "\n",
    "BeautifulSoup allows us to use **CSS queries** to extra the data within specific nodes, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beautiful_page.select('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BeautifulSoup, the **`.select()`** operator finds all of the nodes in an HTML page of a particular type, along with all of their children. \n",
    "\n",
    "Thus, the code above returns all of the **`<table>`** nodes in the document, plus all of the children of the table nodes, like the rows and values in the table.\n",
    "\n",
    "Let's see how many there tables there are on the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(beautiful_page.select('table')))+\" tables on the page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add another argument to the **`.select()`** operator to return only the *rows* of the table.\n",
    "\n",
    "(Remember, rows of table are **`<tr>`** nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beautiful_page.select('table tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more structured. After the first two rows, the rows fall into a regular pattern, where each row corresponds to a **year** and contains data about the top-grossing movie that year.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beautiful_page.select('table tr')[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Each row has several datapoints in it. \n",
    "\n",
    "If we look back at the [original page](http://www.boxofficemojo.com/yearly/), we understand that these datapoints are...\n",
    "- The year\n",
    "- That year's total box office gross\n",
    "- The % change from the previous year\n",
    "- The tickets sold that year\n",
    "- The % change, again\n",
    "- Total # of movies\n",
    "- Total screens\n",
    "- Average cost\n",
    "- #1 Movie\n",
    "\n",
    "Let's take a look at the individual data points by extracting the **`<td>`** nodes, which are the individual values in each row. To do this, we write a **`for`** loop that prints the **`.text`** property of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in beautiful_page.select('table tr')[3].select('td'):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write a loop that extracts all the values from the yearly Box Office page and inserts them into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = beautiful_page.select('table tr')\n",
    "data = []\n",
    "for row in rows:\n",
    "    new_row = []\n",
    "    datapoints = row.select('td')\n",
    "    for datapoint in datapoints:\n",
    "        new_row.append(datapoint.text)\n",
    "    data.append(new_row)\n",
    "\n",
    "#Omitting the first two rows, since they don't follow the pattern\n",
    "data = data[2:len(data)]\n",
    "\n",
    "import pandas\n",
    "yearly_data = pandas.DataFrame(data)\n",
    "\n",
    "#Changing the first line of the dataframe to the column names \n",
    "yearly_data.columns = list(yearly_data.iloc[0])\n",
    "yearly_data.drop(yearly_data.index[0], inplace=True)\n",
    "yearly_data\n",
    "\n",
    "yearly_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a cleaned, organized dataframe, we can start to manipulate datatypes and make some basic charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "yearly_data[\"Year\"] = pandas.to_numeric(yearly_data[\"Year\"])\n",
    "yearly_data[\"# ofMovies\"] = pandas.to_numeric(yearly_data[\"# ofMovies\"])\n",
    "\n",
    "plot.plot(yearly_data[\"Year\"], yearly_data[\"# ofMovies\"])\n",
    "plot.ylabel('Movies Produced')\n",
    "plot.xlabel('Year')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, BeautifulSoup allows us to extract particular datapoints from raw HTML code from anywhere on the internet.\n",
    "\n",
    "But the example above doesn't quite demonstrate the power of web-scraping. After all, we could've just copied and pasted the table above into an Excel document and analyzed it just as easily without writing any code.\n",
    "\n",
    "Where web-scraping becomes powerful and useful is in its ability to automate much larger scale data collection efforts.\n",
    "\n",
    "What if we wanted to extract data on _every_ single movie on BoxOfficeMojo's website? Like the data stored on [this page](http://www.boxofficemojo.com/movies/?id=beautyandthebeast2017.htm):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://static1.squarespace.com/static/58854b3b6a49636e23dfec7d/t/5a32a055652dea6a8a380e32/1513267286491/BoxOffice_ScreenGrab2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many datapoints for individual movies that aren't aggregated neatly on _one_ page, but rather are spread out across _many_ different pages on the website. These datapoints include:\n",
    "\n",
    "- Genre\n",
    "- Director\n",
    "- Foreign gross\n",
    "- Day-by-day gross\n",
    "- Opening weekend\n",
    "- Distributor\n",
    "- Production budget\n",
    "\n",
    "To extract those datapoints, we'll need to be more creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Collection Automation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pulling Data From Individual Pages\n",
    "Each movie in BoxOfficeMojo's database has a separate webpage containing its important data. But each page follows (almost) exactly the same HTML format. \n",
    "\n",
    "This means we can write a single function that extracts the data from _any_ movie's webpage.\n",
    "\n",
    "To facilitate this process, we'll use [Inspector Gadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en). Inspector Gadget is a Chrome plugin that allows us to more easily find the CSS pathways of particular items on a webpage, without looking at the HTML code line by line.\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "![Image](https://static1.squarespace.com/static/58854b3b6a49636e23dfec7d/t/5a37f7e4e4966b79a0e6b19c/1513617381410/Inspect_Page_ScreenGrab2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selector Gadget makes it easy for us to find the CSS pathways of particular parts of a web-page, without having to know much about HTML. We can then use those CSS pathways in conjunction with BeautifulSoup to pull specific datapoints.\n",
    "\n",
    "### Exercise:\n",
    "Use either Selector Gadget or Chrome's inspect page function to write a function that extracts the **title** of the movie from a BoxOffice page. Test it on a few movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTitle(url):\n",
    "    page = BeautifulSoup(requests.get(url, proxies=proxies).text, 'lxml')\n",
    "    return(page.select('body table tr td table tr font b')[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetTitle('http://www.boxofficemojo.com/movies/?id=beautyandthebeast2017.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetTitle('http://www.boxofficemojo.com/movies/?id=starwars2016.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selector Gadget isn't perfect, and often it takes some trial and error to find a pathway that works. \n",
    "\n",
    "Still, it's a useful tool. Now let's write a function that extracts even more data from these webpages.\n",
    "\n",
    "To do this, we'll take a shortcut. Rather than finding the specific CSS pathway for  each datapoint that we want, we can just select all of the **`<b>`** nodes, and then index them in Python to find the ones we want.\n",
    "\n",
    "_(This is an error-prone method, and in practice you should be more precise. But we'll do it this way for the sake of example.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.boxofficemojo.com/movies/?id=starwars8.htm'\n",
    "page = BeautifulSoup(requests.get(url, proxies=proxies).text, 'lxml')\n",
    "index = 0\n",
    "for i in page.select('b'):\n",
    "    print(\"Index: \"+str(index)+\"\\t Text: \"+i.text)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See - every datapoint has an **index** that we can use to build a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = BeautifulSoup(requests.get(url, proxies=proxies).text, 'lxml')\n",
    "links = page.select('b')\n",
    "data = {\n",
    "    \"Title\" : links[1].text,\n",
    "    \"Domestic\" : links[10].text,\n",
    "    \"Worldwide\" : links[13].text,\n",
    "    \"Release\" : links[4].text,\n",
    "    \"Distributor\" : links[3].text, \n",
    "    \"Rating\" : links[7].text,\n",
    "    \"Runtime\" : links[6].text,\n",
    "    \"Genre\"  : links[5].text\n",
    "}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFullData(url):\n",
    "    page = BeautifulSoup(requests.get(url, proxies=proxies, stream=True).text, 'lxml')\n",
    "    links = page.select('b')\n",
    "    data = {\n",
    "        \"Title\" : links[1].text,\n",
    "        \"Domestic\" : links[10].text,\n",
    "        \"Worldwide\" : links[13].text,\n",
    "        \"Release\" : links[4].text,\n",
    "        \"Distributor\" : links[3].text, \n",
    "        \"Rating\" : links[7].text,\n",
    "        \"Runtime\" : links[6].text,\n",
    "        \"Genre\"  : links[5].text\n",
    "    }\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetFullData('http://www.boxofficemojo.com/movies/?id=starwars8.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetFullData('http://www.boxofficemojo.com/movies/?id=beautyandthebeast2017.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Finding Individual Page URLS\n",
    "\n",
    "Now that we have a function that pulls relevant data from individual URLs, we just need  a _list_ of all of the movie URLs to apply it to.\n",
    "\n",
    "A good place to find such lists are the [yearly Box Office pages](http://www.boxofficemojo.com/yearly/chart/?yr=2017&p=.htm) that rank each year's films by box office haul. \n",
    "\n",
    "Each of these pages contains links to exactly 100 movies, and we can easily extract them using, again, BeautifulSoup and Selector Gadget. \n",
    "\n",
    "### Exercise\n",
    "Find all of URLs of individual movies on the [2017 box office leaderboard](http://www.boxofficemojo.com/yearly/chart/?yr=2017&p=.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_url = 'http://www.boxofficemojo.com/yearly/chart/?yr=2017&p=.htm'\n",
    "root = 'http://www.boxofficemojo.com'\n",
    "yearly_page = BeautifulSoup(requests.get(yearly_url, proxies=proxies).text, 'lxml')\n",
    "movies = yearly_page.select('td td b font a')\n",
    "urls = [root + movie.get('href') for movie in movies]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using our function,  we can extract data from all of these URLs.\n",
    "\n",
    "(**Note:** This process takes a few moments. Even though Python processes data very quickly, it still needs to separately query BoxOfficeMojo's server for each individual film, which is a relatively more time-consuming process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = [GetFullData(url) for url in urls]\n",
    "pandas.DataFrame(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it - data on the 100 most succesful movies of 2017, which would've taken hours to collect manually. \n",
    "\n",
    "But there are far more than 100 movies in BoxOfficeMojo's database. What if we wanted to collect data on 10,000 movies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scraping  Across Years and Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the link below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=2017&p=.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how it defines...\n",
    "- `page=1`\n",
    "- `yr=2017`\n",
    "\n",
    "... meaning that the URL leads to the **first page** of movies released  in **2017**.\n",
    "\n",
    "We can use some basic Python concepts - string manipulation and nested loops - to find the same URLs for every page of every year in BoxOfficeMojo's database.\n",
    "\n",
    "### Exercise\n",
    "Write a loop that creates a list of all the URLs for the first five pages of every year leading back to 1985."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1985,2017):\n",
    "    for page in range(1,5):\n",
    "        full_url = 'http://www.boxofficemojo.com/yearly/chart/?page='+str(page)+'&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "        print(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_urls = []\n",
    "for year in range(1985,2017):\n",
    "    for page in range(1,5):\n",
    "        full_url = 'http://www.boxofficemojo.com/yearly/chart/?page='+str(page)+'&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "        full_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of these URLs, we can find the list of _movie_ URLs within each of these pages.\n",
    "\n",
    "_(This will also take a few minutes.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_urls = []\n",
    "for full_url in full_urls:\n",
    "    page = BeautifulSoup(requests.get(full_url, proxies=proxies).text, 'lxml')\n",
    "    movies = page.select('td td b font a')\n",
    "    movie_urls = [root + movie.get('href') for movie in movies]\n",
    "    for movie_url in movie_urls:\n",
    "        all_movie_urls.append(movie_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We've collected \"+str(len(all_movie_urls))+\" movie URLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of URLs for (virtually) every movie in BoxOfficeMojo's website, we're ready to compile a much larger dataset, using roughly the same syntax as before. But before we do, there's a few things we need to keep in mind.\n",
    "\n",
    "First of all, we should understand that having a class full of 30+ people run this code simultaneously could overwhelm BoxOfficeMojo's servers, similar to a [DDOS attack](https://www.digitalattackmap.com/understanding-ddos/) (visualization [here](http://map.norsecorp.com/)). For this reason, it might be best to limit the number of movie URLs in your query, or maybe to try running the code in full later on, by yourself.\n",
    "\n",
    "Second,  it's also a good idea to include some **Error Handling** exceptions in our code. There are two errors we might want to account for. Although _most_ of the movies in the dataset adhere to a uniform HTML format, it's likely that some small percentage of them won't, usually resulting in an **IndexError**. In those cases, we want to make a note of the error, but we don't want to let it interrupt our code. \n",
    "\n",
    "Additionally, by keeping a connection open for long periods of time, we run the risk of receiving a **ChunkedEncodingError** (a type of **ValueError**). When we encounter such an error, we can simply tell our program to wait for a second and then reopen the connection. We can do this using the **`sleep`** method, which works like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "strftime('%X')\n",
    "print(\"Right now it's \"+strftime('%X'))\n",
    "time.sleep(5)\n",
    "print(\"And now it's \"+strftime('%X'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we're ready to begin scraping the entire website. The code below will do just that, _but_ it takes an hour or two to run. During the training, we recommend trying the code on a smaller subset of movies (100 - 200). You can find full output of the code, complete with the data on 10,686 movies, in the *Scraped_Movies.csv* file attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.gmtime()\n",
    "full_movie_data = []\n",
    "error_count = 0 \n",
    "connection_breaks = 0\n",
    "\n",
    "index = 0\n",
    "for all_movie_url in all_movie_urls:\n",
    "    \n",
    "    try:\n",
    "        full_movie_data.append(GetFullData(all_movie_url))\n",
    "\n",
    "    #In case the web-page is formatted differently:\n",
    "    except IndexError:\n",
    "        error_count += 1\n",
    "    \n",
    "    #In case the connection breaks:\n",
    "    except:\n",
    "        connection_breaks\n",
    "        time.sleep(1)\n",
    "        \n",
    "    index += 1\n",
    "\n",
    "        \n",
    "end_time = time.gmtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We collected the data on \"+str(len(full_movie_data))+\" movies.\")\n",
    "print(\"We encountered \"+str(error_count)+\" errors.\")\n",
    "print(\"And it took \" + str(int((time.mktime(end_time) - time.mktime(start_time)) / 60)) +  \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pandas.DataFrame(full_movie_data)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also store thedataset in a CSV so that we can continue to work with it in other exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.to_csv('Scraped_Movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it - data on over 10,000 movies that would've taken days or weeks to compile by hand. This concludes our lesson, but there is a lot more that can accomplished with web-scraping: \n",
    "- Web-scraping programs can be scheduled to run automatically at a particular interval using [crontab](http://www.adminschoice.com/crontab-quick-reference)\n",
    "- Data can be scraped into relational databases, instead of simple flat files like the one above\n",
    "- Since much of the data on the internet isn't strictly quantitative, like the dataset above, we can employ text-processing techniques to process news articles or headlines scraped systematically from the web\n",
    "\n",
    "### Other Useful Resources\n",
    "- [DataQuest CSS Querying and BeautifulSoup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
